{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWPm95Xa7RWy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Aufgabe 1:\n",
        "a) Was sind Neuronale Netzwerke?\n",
        "- rechnergestütze Modelle, die vom menschlichen Gehirn inspiriert sind\n",
        "- bestehen aus mehreren Schichten von input layer bis output layer\n",
        "- jede Schicht besteht aus Neuronen, die Rechenoperationen durchführen um Muster zu erkennen\n",
        "- lernen indem sie die internen Gewichte und Biases anpassen\n",
        "- durch nichtlineare Aktivierungsfunktionen können NNs dabei auch sehr komplexe Funktionen annähern\n",
        "\n",
        "b) Was ist die Chain Rule?\n",
        "- Die chain rule ist ein mathematischer Trick, bei dem man erkennt was eine Veränderung am Anfang (bspw. weight) auf das Ergebnis am Ende (bspw. den Fehler) hat.\n",
        "\n",
        "c) Was versteht man unter dem XOR-Problem? Welche Folgen hatte es und welche Lösungen wurden gefunden?\n",
        "- Ein lineares neuronales kann XOR nicht lernen da es in einem Koordinatensystem nicht mit einer geraden Linie getrennt werden kann\n",
        "- mit mindestens einem hidden layer und einer nichtlinearen Aktivierungsfunktion kann ein neural network XOR lernen\n",
        "\n",
        "d)Beschreibe zwei beliebige Aktivierungsfunktionen.\n",
        "ReLU\n",
        "- 0 wenn z < 0\n",
        "- z wenn z > 0\n",
        "- sehr effizient\n",
        "- funktioniert gut in tiefen Netzen\n",
        "\n",
        "Sigmoid\n",
        "- Werte zwischen 0 und 1\n",
        "- kleine und große eingaben werden abgeflacht\n",
        "\n",
        "e) Wie funktioniert Backpropagation?\n",
        "1. Forward Pass:\n",
        "- Alle Eingaben laufen durchs Netzwerk\n",
        "- Output (Vorhersage) bspw. 0.87\n",
        "2. Loss\n",
        "- Wert zwischen Vorhersage und echtem Wert messen\n",
        "- Das ist der Fehler\n",
        "3. Backward Pass (Chain Rule)\n",
        "- Wie hat jedes Gewicht zum Loss beigetragen\n",
        "- Dafür verwenden wir die Chain Rule\n",
        "- Das passiert für jede Schicht rückwärts, von der letzten bis zur ersten\n",
        "4. Gradient Descent\n",
        "- Das Gewicht wird so angepasst, um den Fehler beim nächsten Mal kleiner zu machen\n"
      ],
      "metadata": {
        "id": "vYmbpnjz7iZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aufgabe 2\n",
        "Skizziere ein simples neuronales Netz, welches die folgenden logischen Ausdrücke repräsentiert.\n",
        "a) (a OR b) AND (c OR d)\n",
        "b) NOT(a OR b) AND (c OR d)\n",
        "c) (a OR b) XOR (c OR d)"
      ],
      "metadata": {
        "id": "WP9DF1UcqsNs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aufgabe 3\n",
        "\n",
        "In dieser Übung wollen wir ein neuronales Netzwerk zur Word-Prediktion erstellen. Dafür nutzen wir PyTorch. Installieren Sie das Package via `pip install torch`.\n",
        "\n",
        "Hierfür soll das Model, ähnlich wie bei N-Grams, anhand des letzten Tokens (in diesem Fall: das letzte Wort) bestimmen, welches das nächste Token (Wort) ist. Bonus: Begrenze die Kontextlänge nicht nur auf das letzte Wort und implementiere eine variable Kontextlänge, zum Beispiel die letzten 3 Wörter.\n",
        "\n",
        "Präparieren Sie einen Datensatz aus den Gutenberg Corpora und nutzen Sie PyTorch, um Ihr Model zu definieren und zu trainieren.\n",
        "\n",
        "Es stehen zwei Notebooks zur Verfügung, wobei `uebung02.ipynb` die Struktur ohne weitere Hilfestellungen vorschlägt und `uebung02_assissted.ipynb` die Definition von Model und Training bereitstellt, sodass Sie sich nur auf die Vorbereitung der Trainingsdaten und die Prediction Schritte konzentrieren können. Das Ziel ist jedoch in beiden Fällen dasselbe.\n",
        "\n",
        "Nutzen Sie zur Definition des NN die Ihnen von der Vorlesung bekannten Elemente: Lineare Layer, eine beliebige Aktivierungsfunktion und Backpropagation im Training.\n",
        "\n",
        "Lassen Sie sich mit dem fertigen Model einige Beispiele für Folgewörter generieren.\n",
        "\n",
        "Für Hilfestellungen, hier ein paar Quellen:\n",
        "Lineare Layer: https://docs.kanaries.net/topics/Python/nn-linear\n",
        "Textgeneration mit RNN: https://abhijoysarkar.blog/2024/05/10/building-a-mini-language-model-with-pytorch-tutorial-walkthrough/ (Achtung: RNN sind noch nicht in der Vorlesung drangekommen - das Model muss entsprechend angepasst werden)\n"
      ],
      "metadata": {
        "id": "xTtKbDOBCxez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "raw_text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "sentences = sent_tokenize(raw_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IzeSlj_B3a-",
        "outputId": "35679051-a56a-4d16-c5fa-fa6c34887c6f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 3\n",
        "\n",
        "allWords = set()\n",
        "document = [\" \".join(word_tokenize(sentence)) for sentence in sentences]\n",
        "for line in document:\n",
        "  words = [\"<start>\"] + line.split() + [\"<end>\"]\n",
        "  allWords.update(words)\n",
        "\n",
        "wtoi = {w:i for i, w in enumerate(sorted(allWords))}\n",
        "itow = {i:w for w, i in wtoi.items()}\n",
        "vocab_size = len(wtoi)"
      ],
      "metadata": {
        "id": "Np-uGnN_GgcS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs = []\n",
        "ys = []\n",
        "\n",
        "for line in document:\n",
        "  words = [\"<start>\"] + line.split() + [\"<end>\"]\n",
        "  for i in range(len(words) - context_size):\n",
        "    context = words[i:i+context_size]\n",
        "    target = words[i+context_size]\n",
        "    xs.append([wtoi[w] for w in context])\n",
        "    ys.append(wtoi[target])\n",
        "\n",
        "inputs_tensor = torch.tensor(xs, dtype=torch.long)\n",
        "outputs_tensor = torch.tensor(ys, dtype=torch.long)"
      ],
      "metadata": {
        "id": "bB_DCR73HLht"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Management beim Training\n",
        "#PyTorch’s Dataset class: allows us to define how to access our data in a format suitable for feeding it into our model\n",
        "# DataLoader handles batching of data, shuffling, and parallel processing.\n",
        "\n",
        "# Combine into dataset\n",
        "class WordDataset(Dataset):\n",
        "    def __init__(self, xs, ys):\n",
        "        self.xs = xs\n",
        "        self.ys = ys\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.xs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.xs[index], self.ys[index]\n",
        "\n",
        "dataset = WordDataset(inputs_tensor, outputs_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "V0nf3z7NCDE-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordPredictor(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, context_size):\n",
        "        # Definiert die Struktur vom Predictor\n",
        "        super().__init__()\n",
        "        # Embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        # Layer 1: Linear\n",
        "        self.fc1 = nn.Linear(embed_dim * context_size, hidden_dim)\n",
        "        # Activation Function: ReLu\n",
        "        self.relu = nn.ReLU()\n",
        "        # Layer 2: Linear\n",
        "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Methode wird immer aufgerufen, wenn Predictions gemacht werden (Training und Inferenz)\"\"\"\n",
        "\n",
        "        # Step 1: Wort-Indexe zu Embeddings konvertieren und glätten -> Kontext Wörter in einen Vektor, der vom linearen Layer verwendet werden kann\n",
        "        x = self.embedding(x).view(x.size(0), -1)\n",
        "\n",
        "        # Step 2: Durch das 1. Layer -> Lineare Transformation, um Inputs in das versteckte Layer zu projizieren\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        # Step 3: ReLU activation function\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Step 4: 2. fully connected layer -> Wieder auf die Vokabelgröße projizieren\n",
        "        return self.fc2(x)\n",
        "\n",
        "model = WordPredictor(vocab_size, embed_dim=64, hidden_dim=128, context_size=context_size)"
      ],
      "metadata": {
        "id": "yiYg5DTICjzv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function -> Misst wie gut die Model Prediction dem richtigen Output entspricht\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Optimizer -> Updatet die Gewichte und Parameter um Loss ^ zu minimieren (in dem Fall Gradient Descent)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "Tdnn8tfCCnhi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_epochs = 10\n",
        "\n",
        "# Durchgehen durch die Trainingsepochen\n",
        "for epoch in range(no_of_epochs):\n",
        "    # Loss für Dokumentation\n",
        "    total_loss = 0\n",
        "    for batch_x, batch_y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(batch_x)\n",
        "\n",
        "        # Loss berechnen\n",
        "        loss = criterion(logits, batch_y)\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YieRhaXICpSm",
        "outputId": "7d7159da-23ef-489a-aace-cbacfff6fb2b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 30808.8581\n",
            "Epoch 2, Loss: 26802.3455\n",
            "Epoch 3, Loss: 25075.0818\n",
            "Epoch 4, Loss: 23941.0579\n",
            "Epoch 5, Loss: 23069.5812\n",
            "Epoch 6, Loss: 22341.0500\n",
            "Epoch 7, Loss: 21732.0623\n",
            "Epoch 8, Loss: 21182.2555\n",
            "Epoch 9, Loss: 20698.2884\n",
            "Epoch 10, Loss: 20255.1737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentence(model, start_words, length=10):\n",
        "    model.eval()\n",
        "    words = start_words[:]\n",
        "    for _ in range(length):\n",
        "        context = words[-context_size:]\n",
        "        # Padding falls zu kurz\n",
        "        if len(context) < context_size:\n",
        "            context = [\"<start>\"] * (context_size - len(context)) + context\n",
        "        context_ids = [wtoi.get(w, wtoi[\"<start>\"]) for w in context]\n",
        "        x = torch.tensor([context_ids], dtype=torch.long)\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            next_word_idx = torch.argmax(logits, dim=1).item()\n",
        "            next_word = itow[next_word_idx]\n",
        "        words.append(next_word)\n",
        "        if next_word == \"<end>\":\n",
        "            break\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Beispiel:\n",
        "start = [\"she\", \"was\", \"very\"]\n",
        "print(\"Generiert:\", generate_sentence(model, start))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlHahRj3RRCD",
        "outputId": "4e71cd01-3899-4e29-b0fe-22bee529b095"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generiert: she was very much pleased with the first , and the same party\n"
          ]
        }
      ]
    }
  ]
}